{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXPERIMENT 1: \n",
    "# Increased the number of compnents from 1 to 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTAL_CLASSES = 1103\n",
    "TOTAL_CLASSES = 1103 #reducing the number of labels to 398 (only culture tags)\n",
    "\n",
    "# DATA_PATH_SMALL = \"../data/train\"\n",
    "DATA_PATH_BIG = \"../data/train/\"\n",
    "device = 'cuda'\n",
    "z_dim = 64 #noise vector dimension\n",
    "\n",
    "image_resize = 28\n",
    "postcard_shape = (3, image_resize, image_resize) \n",
    "n_classes = TOTAL_CLASSES\n",
    "\n",
    "criterion_name = \"BCEWithLogitsLoss\" \n",
    "n_epochs = 10\n",
    "display_step = 20\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "torch.manual_seed(0) # Set for our testing purposes, please do not change!\n",
    "from PIL import Image\n",
    "import csv\n",
    "\n",
    "if criterion_name == \"BCEWithLogitsLoss\":\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "  raise ValueError(\"Criterion is not defined\")\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConvTranspose2d(nn.Module):\n",
    "    def __init__(self, conv, output_size):\n",
    "        super(MyConvTranspose2d, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.conv = conv\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x, output_size=self.output_size)\n",
    "        return x\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
    "              (MNIST is black-and-white, so 1 channel is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, input_dim=10, im_chan=3, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(input_dim, hidden_dim * 4, kernel_size=3, stride=2),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 3, kernel_size=4, stride=1),\n",
    "            self.make_gen_block(hidden_dim * 3, hidden_dim * 2, kernel_size=5, stride=1),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim * 1, kernel_size=6, stride=1),\n",
    "            self.make_gen_block(hidden_dim * 1, hidden_dim * 1, kernel_size=7, stride=1),\n",
    "            self.make_gen_block(hidden_dim * 1, im_chan, kernel_size=8, stride=1, final_layer=True)\n",
    ")\n",
    "    \n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, input_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "def get_noise(n_samples, input_dim, device='cpu'):\n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        device: the device type\n",
    "    '''\n",
    "    return torch.randn(n_samples, input_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "      im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
    "            (MNIST is black-and-white, so 1 channel is your default)\n",
    "      hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, im_chan=3, hidden_dim=64): #changing im_chan\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self.make_disc_block(im_chan, hidden_dim),\n",
    "            self.make_disc_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_disc_block(hidden_dim * 2, 1, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_disc_block(self, input_channels, output_channels, kernel_size=2, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; \n",
    "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the discriminator: Given an image tensor, \n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with dimension (im_chan)\n",
    "        '''\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def get_one_hot_labels(labels, n_classes):\n",
    "    '''\n",
    "    Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes).\n",
    "    Parameters:\n",
    "        labels: tensor of labels from the dataloader, size (?)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "    '''\n",
    "    #### START CODE HERE ####\n",
    "    return nn.functional.one_hot(labels, n_classes) \n",
    "    #### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_vectors(x, y):\n",
    "    '''\n",
    "    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?).\n",
    "    Parameters:\n",
    "      x: (n_samples, ?) the first vector. \n",
    "        In this assignment, this will be the noise vector of shape (n_samples, z_dim), \n",
    "        but you shouldn't need to know the second dimension's size.\n",
    "      y: (n_samples, ?) the second vector.\n",
    "        Once again, in this assignment this will be the one-hot class vector \n",
    "        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.\n",
    "    '''\n",
    "    # Note: Make sure this function outputs a float no matter what inputs it receives\n",
    "    #### START CODE HERE ####\n",
    "#     print(x.shape, y.shape)\n",
    "    combined = torch.cat((x.float(),y.float()),dim=1)\n",
    "    #### END CODE HERE ####\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Dataset class for dsprites\"\"\"\n",
    "\n",
    "    def __init__(self, data_root, normalize=True, rotate=False):\n",
    "        \n",
    "        #get the data labels\n",
    "        self.img_to_labels = {}\n",
    "        with open(DATA_PATH_BIG+'train.csv') as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            next(csv_reader) # skips the first row containing the row names\n",
    "            for row in tqdm(csv_reader):\n",
    "              classes = row[1].split()\n",
    "              classes = [int(e) for e in classes]\n",
    "              self.img_to_labels[str(row[0])] = classes\n",
    "\n",
    "        # Recursively exract paths to all .png files in subdirectories\n",
    "        self.file_paths = []\n",
    "        self.file_names = []\n",
    "        for path, subdirs, files in tqdm(os.walk(data_root)):\n",
    "            for name in files:\n",
    "                plain_name = name.split('.')[0]\n",
    "\n",
    "                if name.endswith(\".png\") and plain_name in self.img_to_labels:\n",
    "                    \n",
    "                    # print('here')\n",
    "                    # Way 1\n",
    "                    # condition = True\n",
    "                    # for class_ in self.img_to_labels[plain_name]:\n",
    "                    #     if class_ >= TOTAL_CLASSES:\n",
    "                    #         condition = False\n",
    "                    #         break\n",
    "\n",
    "                    # Way 2\n",
    "                    condition = True\n",
    "                    labels = self.img_to_labels[plain_name]\n",
    "                    filtered_labels = [e for e in labels if e < TOTAL_CLASSES]\n",
    "                    if len(filtered_labels) == 0:\n",
    "                        condition = False\n",
    "\n",
    "                    if condition:\n",
    "                      self.file_paths.append(os.path.join(path, name))\n",
    "                      name = plain_name\n",
    "                      self.file_names.append(name)\n",
    "            # break\n",
    "        self.transform = self._set_transforms(normalize, rotate)\n",
    "\n",
    "\n",
    "    def _set_transforms(self, normalize, rotate):\n",
    "        \"\"\"Decide transformations to data to be applied\"\"\"\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "                    transforms.Resize((image_resize,image_resize)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5,), (0.5,)),\n",
    "                    ])\n",
    "        return transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Required: specify dataset length for dataloader\"\"\"\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Required: specify what each iteration in dataloader yields\"\"\"\n",
    "        img = Image.open(self.file_paths[index])\n",
    "        img = self.transform(img)\n",
    "        one_hot = torch.zeros(n_classes, dtype=int)\n",
    "        classes = self.img_to_labels[self.file_names[index]]\n",
    "\n",
    "        for class_ in classes:\n",
    "          one_hot[class_]=1\n",
    "        \n",
    "        return img, one_hot\n",
    "\n",
    "# dataset = Dataset(data_root=DATA_PATH_SMALL+'train/')\n",
    "\n",
    "#loading the bigger dataset\n",
    "dataset = Dataset(data_root=DATA_PATH_BIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = data.DataLoader(dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=4,\n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing if dataloader works\n",
    "for real,labels in tqdm(dataloader):\n",
    "  cur_batch_size = len(real)\n",
    "  real = real.to(device='cpu')\n",
    "  print(real.shape) #currently have 101 images\n",
    "  print('hi')\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_dimensions(z_dim, postcard_shape, n_classes):\n",
    "    '''\n",
    "    Function for getting the size of the conditional input dimensions \n",
    "    from z_dim, the image shape, and number of classes.\n",
    "    Parameters:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        postcard_shape: the shape of each postcard image as (C, W, H), which is (3, 200, 200)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "                (10 for MNIST)\n",
    "    Returns: \n",
    "        generator_input_dim: the input dimensionality of the conditional generator, \n",
    "                          which takes the noise and class vectors\n",
    "        discriminator_im_chan: the number of input channels to the discriminator\n",
    "                            (e.g. C x 200 x 200 for postcard)\n",
    "    '''\n",
    "    #### START CODE HERE ####\n",
    "    generator_input_dim = z_dim+n_classes\n",
    "    discriminator_im_chan = postcard_shape[0]*(n_classes+1)\n",
    "    # discriminator_im_chan = postcard_shape[0]+n_classes\n",
    "\n",
    "    #### END CODE HERE ####\n",
    "    return generator_input_dim, discriminator_im_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, postcard_shape, n_classes)\n",
    "\n",
    "gen = Generator(input_dim=generator_input_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "disc = Discriminator(im_chan=discriminator_im_chan).to(device)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-grade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator_input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-vatican",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-paris",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "#UNIT TEST NOTE: Initializations needed for grading\n",
    "noise_and_labels = False\n",
    "fake = False\n",
    "\n",
    "fake_image_and_labels = False\n",
    "real_image_and_labels = False\n",
    "disc_fake_pred = False\n",
    "disc_real_pred = False\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches and the labels\n",
    "    for real, labels in tqdm(dataloader):\n",
    "\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.to(device)\n",
    "        # one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)\n",
    "        one_hot_labels = labels.to(device)\n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(1, postcard_shape[0], postcard_shape[1], postcard_shape[2])\n",
    "        # image_one_hot_labels = image_one_hot_labels.repeat(1, postcard_shape[0], 28, 28) #ToDo \n",
    "\n",
    "        ### Update discriminator ###\n",
    "        # Zero out the discriminator gradients\n",
    "        disc_opt.zero_grad()\n",
    "        # Get noise corresponding to the current batch_size \n",
    "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "        \n",
    "        # Now you can get the images from the generator\n",
    "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
    "        #        2) Generate the conditioned fake images\n",
    "       \n",
    "        #### START CODE HERE ####\n",
    "        noise_and_labels = combine_vectors(fake_noise, one_hot_labels)\n",
    "#         print(noise_and_labels.shape)\n",
    "        fake = gen(noise_and_labels)\n",
    "\n",
    "        #### END CODE HERE ####\n",
    "        \n",
    "        # Make sure that enough images were generated\n",
    "        assert len(fake) == len(real)\n",
    "        # Check that correct tensors were combined\n",
    "        assert tuple(noise_and_labels.shape) == (cur_batch_size, fake_noise.shape[1] + one_hot_labels.shape[1])\n",
    "        # It comes from the correct generator\n",
    "#         print('fake shape: ', fake.shape)\n",
    "        assert tuple(fake.shape) == (len(real), 3, image_resize, image_resize) #ToDo: check the fake image size\n",
    "        # Now you can get the predictions from the discriminator\n",
    "        # Steps: 1) Create the input for the discriminator\n",
    "        #           a) Combine the fake images with image_one_hot_labels, \n",
    "        #              remember to detach the generator (.detach()) so you do not backpropagate through it\n",
    "        #           b) Combine the real images with image_one_hot_labels\n",
    "        #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n",
    "        #        3) Get the discriminator's prediction on the reals as disc_real_pred\n",
    "        \n",
    "        #### START CODE HERE ####\n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
    "        real_image_and_labels = combine_vectors(real, image_one_hot_labels)\n",
    "\n",
    "        disc_fake_pred = disc(fake_image_and_labels.detach())\n",
    "        disc_real_pred = disc(real_image_and_labels)\n",
    "        #### END CODE HERE ####\n",
    "        \n",
    "        # Make sure shapes are correct \n",
    "        assert tuple(fake_image_and_labels.shape) == (len(real), fake.detach().shape[1] + image_one_hot_labels.shape[1], image_resize ,image_resize)\n",
    "        assert tuple(real_image_and_labels.shape) == (len(real), real.shape[1] + image_one_hot_labels.shape[1],image_resize , image_resize)\n",
    "        # Make sure that enough predictions were made\n",
    "        assert len(disc_real_pred) == len(real)\n",
    "        # Make sure that the inputs are different\n",
    "        assert torch.any(fake_image_and_labels != real_image_and_labels)\n",
    "        # Shapes must match\n",
    "        assert tuple(fake_image_and_labels.shape) == tuple(real_image_and_labels.shape)\n",
    "        assert tuple(disc_fake_pred.shape) == tuple(disc_real_pred.shape)\n",
    "        \n",
    "        \n",
    "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred))\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        disc_opt.step() \n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        discriminator_losses += [disc_loss.item()]\n",
    "\n",
    "        ### Update generator ###\n",
    "        # Zero out the generator gradients\n",
    "        gen_opt.zero_grad()\n",
    "\n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
    "        # This will error if you didn't concatenate your labels to your image correctly\n",
    "        disc_fake_pred = disc(fake_image_and_labels)\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "        \n",
    "\n",
    "        # Keep track of the generator losses\n",
    "        generator_losses += [gen_loss.item()]\n",
    "        #\n",
    "\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
    "            print(f\"Epoch {epoch} Step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\")\n",
    "            show_tensor_images(fake)\n",
    "            show_tensor_images(real)\n",
    "            step_bins = 20\n",
    "            x_axis = sorted([i * step_bins for i in range(len(generator_losses) // step_bins)] * step_bins)\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Generator Loss\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Discriminator Loss\"\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        elif cur_step == 0:\n",
    "            print(\"Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\")\n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-today",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-contrary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
